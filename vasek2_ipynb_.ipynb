{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aUlxnm7cnWy"
      },
      "source": [
        "## Set up the Colab instance to run on a GPU accelerator\n",
        "\n",
        "\n",
        "Navigate to Edit→Notebook Settings and select \"GPU\" from the \"Hardware accelerator\" drop-down menu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUyqKSAWRGNw"
      },
      "source": [
        "## Install dependencies, download the model, set up your PYTHONPATH\n",
        "\n",
        "From here on, you'll start seeing a mix of code and Linux system commands. System commands are prefixed by a shebang `!`, which tells this notebook to execute them on the command line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtNnMxtte0EF"
      },
      "source": [
        "### Install required Python packages\n",
        "\n",
        "This may take 2-3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMEkgpy6T0pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73c10a5-654a-48a1-d2a4-27a8c7138fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: humanfriendly in /usr/local/lib/python3.10/dist-packages (10.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.0.190)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install humanfriendly jsonpickle ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXn_-PZqTWB4"
      },
      "source": [
        "### Download the MegaDetector model files\n",
        "\n",
        "We'll download both MegaDetector v5a and v5b.  See the [release notes](https://github.com/agentmorris/MegaDetector/releases/tag/v5.0) for information about the differences between the two models.\n",
        "\n",
        "We have to use the not-quite-official releases of each, because the version of PyTorch supported by MD is no longer available via pip, and conda is... difficult in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5uwmpmaTZMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359cef0d-21ea-4629-a31b-b576449a3011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 16:14:23--  https://lila.science/public/md_rebuild/md_v5a.0.0_rebuild_pt-1.12_zerolr.pt\n",
            "Resolving lila.science (lila.science)... 20.83.252.133\n",
            "Connecting to lila.science (lila.science)|20.83.252.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 281461925 (268M)\n",
            "Saving to: ‘/content/md_v5a.0.0.pt’\n",
            "\n",
            "/content/md_v5a.0.0 100%[===================>] 268.42M  86.0MB/s    in 3.4s    \n",
            "\n",
            "2023-09-30 16:14:26 (79.0 MB/s) - ‘/content/md_v5a.0.0.pt’ saved [281461925/281461925]\n",
            "\n",
            "--2023-09-30 16:14:26--  https://lila.science/public/md_rebuild/md_v5b.0.0_rebuild_pt-1.12_zerolr.pt\n",
            "Resolving lila.science (lila.science)... 20.83.252.133\n",
            "Connecting to lila.science (lila.science)|20.83.252.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 281461925 (268M)\n",
            "Saving to: ‘/content/md_v5b.0.0.pt’\n",
            "\n",
            "/content/md_v5b.0.0 100%[===================>] 268.42M  79.3MB/s    in 3.7s    \n",
            "\n",
            "2023-09-30 16:14:30 (73.1 MB/s) - ‘/content/md_v5b.0.0.pt’ saved [281461925/281461925]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O /content/md_v5a.0.0.pt https://lila.science/public/md_rebuild/md_v5a.0.0_rebuild_pt-1.12_zerolr.pt\n",
        "!wget -O /content/md_v5b.0.0.pt https://lila.science/public/md_rebuild/md_v5b.0.0_rebuild_pt-1.12_zerolr.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJ6lQX8S4im"
      },
      "source": [
        "### Clone the required git repos\n",
        "This will copy the latest version of the MegaDetector and YOLOv5 repos, which are required to run MegaDetector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qhltAaRSe1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e263da78-06db-4380-a736-b103330c55ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/MegaDetector'...\n",
            "remote: Enumerating objects: 17246, done.\u001b[K\n",
            "remote: Counting objects: 100% (2092/2092), done.\u001b[K\n",
            "remote: Compressing objects: 100% (719/719), done.\u001b[K\n",
            "remote: Total 17246 (delta 1381), reused 2079 (delta 1370), pack-reused 15154\u001b[K\n",
            "Receiving objects: 100% (17246/17246), 187.14 MiB | 33.15 MiB/s, done.\n",
            "Resolving deltas: 100% (10577/10577), done.\n",
            "Cloning into '/content/yolov5'...\n",
            "remote: Enumerating objects: 16000, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 16000 (delta 20), reused 20 (delta 12), pack-reused 15967\u001b[K\n",
            "Receiving objects: 100% (16000/16000), 14.59 MiB | 18.82 MiB/s, done.\n",
            "Resolving deltas: 100% (10986/10986), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/MegaDetector\n",
        "!rm -rf /content/yolov5\n",
        "!git clone https://github.com/agentmorris/MegaDetector /content/MegaDetector\n",
        "!git clone https://github.com/ultralytics/yolov5 /content/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pzfM5Y-iby1"
      },
      "source": [
        "### Set `PYTHONPATH` to include `MegaDetector` and `yolov5`\n",
        "\n",
        "Add cloned git folders to the `PYTHONPATH` environment variable so that we can import their modules from any working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8vanlgAOlEj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/MegaDetector\"\n",
        "os.environ['PYTHONPATH'] += \":/content/yolov5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYsrTTR7eF0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a81d2da-efa9-45a6-f422-026613049946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/train_dataset_altai.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoHowoGs28oQ",
        "outputId": "9a7e1abe-d59d-42a7-d5b6-a94ee2312dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_dataset_altai.zip\n",
            "replace broken_imgs/PICT0082.JPG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkugt7r3uUEr"
      },
      "source": [
        "## MegaDetector batch processing\n",
        "\n",
        "This step executes the Python script `run_detector_batch.py` from the MegaDetector repo. It has three mandatory arguments and one optional:\n",
        "\n",
        "1. Path to the MegaDetector model file\n",
        "2. A folder containing images.  This notebook points to the folder where we just put our Snapshot Serengeti images; if your images were already on Google Drive, replace `[Image_Folder]` with your folder name.\n",
        "3. The output JSON file location and name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSIH-k0nfi73"
      },
      "outputs": [],
      "source": [
        "images_dir = '/content/Фотоловушка Иониха'\n",
        "\n",
        "# Choose a location for the output JSON file\n",
        "output_file_path = '/content/drive/My Drive/fotolovuska/results.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YZs9wT1sAgV"
      },
      "source": [
        "# Run the detection script\n",
        "\n",
        "There are actually two variants of MegaDetector v5, called \"v5a\" and \"v5b\".  By default this notebook runs MDv5a; change \"md_v5a.0.0.pt\" to \"md_v5b.0.0.pt\" to run MDv5b instead.\n",
        "\n",
        "Both run at the same speed; if you are in a Colab session with a GPU accelerator, you should be able to process around four images per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AOKfviGuTNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d422899-4ec3-4c6a-ce98-01eb730f140a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420 image files found in the input directory\n",
            "PyTorch reports 1 available CUDA devices\n",
            "GPU available: True\n",
            "Imported YOLOv5 from PYTHONPATH\n",
            "Using PyTorch version 2.0.1+cu118\n",
            "Fusing layers... \n",
            "Model summary: 574 layers, 139990096 parameters, 0 gradients, 207.9 GFLOPs\n",
            "Sending model to GPU\n",
            "Loaded model in 16.95 seconds\n",
            "Loaded model in 16.95 seconds\n",
            "100% 420/420 [01:40<00:00,  4.19it/s]\n",
            "Finished inference for 420 images in 2 minutes and 1.38 seconds (3.46 images per second)\n",
            "Output file saved at /content/drive/My Drive/fotolovuska/results.json\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!python /content/MegaDetector/detection/run_detector_batch.py md_v5a.0.0.pt \"$images_dir\" \"$output_file_path\" --recursive --output_relative_filenames --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en3TbCftkWDE"
      },
      "outputs": [],
      "source": [
        "# Render bounding boxes on our images\n",
        "visualization_dir = '/content/visualized_images'\n",
        "!python /content/MegaDetector/md_visualization/visualize_detector_output.py \"$output_file_path\" \"$visualization_dir\" --confidence 0.2 --images_dir \"$images_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AglNEK0goyjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "88cf342e-0d23-47e3-c8dc-9ba25799e0fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-144c58f19cd0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mviz_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualization_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviz_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'visualization_dir' is not defined"
          ]
        }
      ],
      "source": [
        "# Show the images with bounding boxes in Colab\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "for viz_file_name in os.listdir(visualization_dir):\n",
        "  print(viz_file_name)\n",
        "  im = Image.open(os.path.join(visualization_dir, viz_file_name))\n",
        "  display(im)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from PIL import Image as ImagePIL\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "# Открытие файла для чтения\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "path_in= output_file_path\n",
        "path_out='/content/drive/My Drive/fotolovuska/result.csv'"
      ],
      "metadata": {
        "id": "hyyE_8GU4yT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD_BLUR = 200\n",
        "THRESHOLD_IDENTICAL_BYTES = 65000\n",
        "RESIZE_SCALE_FACTOR = 0.1\n",
        "\n",
        "BROKEN_DIR = \"broken\"\n",
        "EMPTY_DIR = \"empty\"\n",
        "ANIMAL_DIR = \"animal\"\n",
        "\n",
        "def check_file_size(filename):\n",
        "    filesize = os.stat(filename).st_size\n",
        "    if os.stat(filename).st_size:\n",
        "        raise SyntaxError(\"Zero size file\")\n",
        "    return filesize\n",
        "\n",
        "def test_PIL(filename):\n",
        "    img = ImagePIL.open(filename)\n",
        "    img.verify()  # используем для проверки встроенную ф-ю verify()\n",
        "    img.close()\n",
        "\n",
        "    img = ImagePIL.open(filename)\n",
        "    img.transpose(ImagePIL.FLIP_LEFT_RIGHT) # делаем преобразование изображение. Выдает ошибку если есть некоторые дефекты в файле\n",
        "    img.close()\n",
        "\n",
        "def calc_number_identical_bytes(filename):\n",
        "    f = open(filename, \"rb\")\n",
        "    bin_data = f.read()\n",
        "    f.close()\n",
        "    n = 1\n",
        "    maxnum = 0\n",
        "    prev = None\n",
        "\n",
        "    for i in bin_data:\n",
        "        if prev == i:\n",
        "            n += 1\n",
        "        else:\n",
        "            if n > maxnum:\n",
        "                maxnum = n\n",
        "\n",
        "            n = 1\n",
        "            prev = i\n",
        "    if n > maxnum:\n",
        "        maxnum = n\n",
        "\n",
        "    return maxnum\n",
        "\n",
        "def bad_color_areas(gray_image): #функция искать одноцветные области брака\n",
        "    b=0\n",
        "    n=np.array(gray_image)\n",
        "    unique, counts = np.unique(np.ravel(gray_image), return_counts=True)\n",
        "\n",
        "    for i in range(len(n[:,0])):\n",
        "        if len(np.unique(np.array(n[i,:])))==1 :\n",
        "            uno=np.unique(np.array(n[i,:]), return_counts=False)\n",
        "            if uno!=0:\n",
        "                if uno!=255:\n",
        "                    b+=1\n",
        "\n",
        "    #print(b)\n",
        "    br=0\n",
        "    if b>25:\n",
        "        br=1\n",
        "\n",
        "    elif len(np.array(unique))<7:\n",
        "        br=1\n",
        "    elif (max(np.array(counts))/len(np.ravel(gray_image)))>0.25:\n",
        "\n",
        "        if np.mean(n)<240:\n",
        "            br=0\n",
        "            if np.mean(n)<70:\n",
        "                br=0\n",
        "            else: br=1\n",
        "\n",
        "        else: br=1\n",
        "        # print(max(np.array(counts))/len(n))\n",
        "        #  br=1\n",
        "\n",
        "    return br\n",
        "\n",
        "def koeff(gray_image):#соотношения белого, черного, серединки\n",
        "    n=np.ravel(gray_image)\n",
        "    av=np.mean(n)\n",
        "    koef_bl=sum(n>240)/len(n)\n",
        "    koef_wt=sum(n<10)/len(n)\n",
        "    koef_av=(sum(n<(av+10))-sum(n<(av-10)))/len(n)\n",
        "    return (av,koef_bl,koef_wt,koef_av)\n",
        "\n",
        "def color_channel_bad(img): #ошибки в цветовых каналах\n",
        "    r = img[:,:,0]\n",
        "    g= img[:,:,1]\n",
        "    b= img[:,:,2]\n",
        "    rr=np.ravel(r)\n",
        "    gr=np.ravel(g)\n",
        "    br=np.ravel(b)\n",
        "    #print(np.mean(rr),np.mean(br),np.mean(gr))\n",
        "    if np.mean(rr)>2*np.mean(gr) or np.mean(rr)>2*sum(br):\n",
        "        br=1\n",
        "    elif np.mean(gr)>2*np.mean(rr) or np.mean(gr)>2*np.mean(br):\n",
        "        br=1\n",
        "    elif np.mean(br)>2*np.mean(gr) or np.mean(br)>2*np.mean(rr):\n",
        "        br=1\n",
        "    else:\n",
        "        br=0\n",
        "    return br\n",
        "\n",
        "\n",
        "def variance_of_laplacian(image):\n",
        "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
        "\n",
        "def copy_file(full_filename, target_dir, sub_dir):\n",
        "    full_dir = os.path.join(target_dir, sub_dir)\n",
        "    os.makedirs(full_dir, exist_ok=True)\n",
        "    shutil.copy2(full_filename, full_dir)\n",
        "\n",
        "\n",
        "def processing_dataset(srcpath, targetpath, jsonpath):\n",
        "\n",
        "    # TODO добавить проверку на существование уже таких целевых папок\n",
        "    full_broken_dir = os.path.join(targetpath, BROKEN_DIR)\n",
        "    full_empty_dir = os.path.join(targetpath, EMPTY_DIR)\n",
        "    full_animal_dir = os.path.join(targetpath, ANIMAL_DIR)\n",
        "\n",
        "    # Создаем директории для каждого класса\n",
        "    os.makedirs(full_broken_dir, exist_ok=True)\n",
        "    os.makedirs(full_empty_dir, exist_ok=True)\n",
        "    os.makedirs(full_animal_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    with open(jsonpath, 'r') as file:\n",
        "        # Parse JSON data\n",
        "        jsondata = json.load(file)\n",
        "\n",
        "\n",
        "    csv_data = []\n",
        "    num_file = len(jsondata['images'])\n",
        "    current_count = 0\n",
        "    break_count = -1 # для отладки установить сколько строк обработать в json файле или -1 для всех\n",
        "\n",
        "    for json_image in jsondata['images'] :\n",
        "        if not break_count:\n",
        "            break\n",
        "        break_count -= 1\n",
        "        current_count += 1\n",
        "        #path = root.split(os.sep)\n",
        "        img_filename_json = json_image['file']\n",
        "        # отделяем относительный путь от имени файла изображения\n",
        "        path, file = os.path.split(img_filename_json)\n",
        "\n",
        "        full_filename = os.path.join(srcpath, img_filename_json)\n",
        "\n",
        "        print(full_filename)\n",
        "\n",
        "        num_animals=0\n",
        "        detections = json_image['detections']\n",
        "        for i in range(len(detections)):\n",
        "            kategoria=(detections[i]['category'])\n",
        "            uverennost= (detections[i]['conf'])\n",
        "            if int(kategoria) == 1 and float(uverennost)>0.7:\n",
        "                num_animals += 1\n",
        "                print(\"{}: {:.2f}\".format(\"Обнаружены животные с уверенностью\", uverennost))\n",
        "            #i+=1\n",
        "\n",
        "        if num_animals > 0:\n",
        "            copy_file(full_filename, full_animal_dir, path) # копируем файл в директорию животных\n",
        "            csv_data.append([img_filename_json,0,0,1])\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Техническая проверка возможности считывания файла и работы с ним библиотекой PIL\n",
        "            test_PIL(full_filename)\n",
        "        except:\n",
        "            print(\"test_PIL: обнаружена ошибка в файле!\")\n",
        "            copy_file(full_filename, full_broken_dir, path) # копируем файл в директорию сломанных\n",
        "            csv_data.append([img_filename_json,1,0,0])\n",
        "        else: # если файл нормально считывается\n",
        "\n",
        "            image = cv2.imdecode(np.fromfile(full_filename, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "            #image = cv2.imread(full_filename)\n",
        "            # TODO знась ОЧЕНЬ НУЖНО выловить сообщения об ошибке вида:\n",
        "            # Corrupt JPEG data:\n",
        "            # .imread() посылает их в поток сообщений, но не вызывает никаких ошибок и исключений!\n",
        "            # это решит проблему с частично битыми изображениями, где остальные методы не сработали\n",
        "\n",
        "            # Наиболее частая проблема в датасете - это размытые и засвеченные кадры\n",
        "            # Проанализируем размытость изображения, для этого сперва преобразуем изображение в градации серого\n",
        "\n",
        "\n",
        "            if len(image.shape) == 2: # проверка если изображение с одним каналом\n",
        "                gray_image = image\n",
        "            else: # если не делать проверку, то ч/б изображения вызывают ошибку здесь\n",
        "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # уменьшаем размер\n",
        "            image = cv2.resize(image,(0, 0), fx=RESIZE_SCALE_FACTOR, fy=RESIZE_SCALE_FACTOR)\n",
        "\n",
        "            # сворачиваем изображение с помощью следующего ядра, размерностью 3х3\n",
        "            # 0  1  0\n",
        "            # 1 -4  1\n",
        "            # 0  1  0\n",
        "            # и вычислим дисперсию Лапласа, чем она меньше, тем более размытое изображение\n",
        "            # Оператор Лапласа выделяет области изображения, содержащие быстрые изменения интенсивности\n",
        "\n",
        "            fm = variance_of_laplacian(gray_image)\n",
        "\n",
        "            if fm < THRESHOLD_BLUR:\n",
        "                text = \"Blurry\"\n",
        "                print(\"{}: {:.2f}\".format(\"Размытое изображение variance_of_laplacian\", fm))\n",
        "\n",
        "                copy_file(full_filename, full_broken_dir, path) # копируем файл в директорию сломанных\n",
        "                csv_data.append([img_filename_json,1,0,0])\n",
        "                continue\n",
        "            else:\n",
        "                text = \"Not Blurry\"\n",
        "\n",
        "            # Проверим, файл на бинарном уровне на наличие длинных повторяющихся последовательностей (битый канал или однородный цвет)\n",
        "\n",
        "            number_identical_bytes = calc_number_identical_bytes(full_filename)\n",
        "            if number_identical_bytes > THRESHOLD_IDENTICAL_BYTES: # если количество повторяющихся байт в файле больше заданного порога\n",
        "                print(\"{}: {:d}\".format(\"Вероятно поврежденный файл! calc_number_identical_bytes \", number_identical_bytes))\n",
        "                copy_file(full_filename, full_broken_dir, path)# копируем файл в директорию сломанных\n",
        "                csv_data.append([img_filename_json,1,0,0])\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # здесь идеи Кати\n",
        "            br = 0\n",
        "            av, koef_bl, koef_wt, koef_av = koeff(gray_image)\n",
        "            if av <=10 or av>=240:\n",
        "                br=1\n",
        "                print(\"Broken: av <=10 or av>=240\")\n",
        "            elif koef_bl>0.8 or koef_av>0.8 or koef_wt>0.8:\n",
        "                br=1\n",
        "                print(\"Broken: koef_bl>0.8 or koef_av>0.8 or koef_wt>0.8\")\n",
        "            else :\n",
        "                br = bad_color_areas(gray_image)\n",
        "                print(\"bad_color_areas = \"+str(br))\n",
        "                if br == 0: # след проверка\n",
        "                    br = color_channel_bad(image)\n",
        "                    print(\"color_channel_bad = \"+str(br))\n",
        "\n",
        "            if br:\n",
        "                print(\"Помещаем в класс Broken.\")\n",
        "                copy_file(full_filename, full_broken_dir, path) # копируем файл в директорию сломанных\n",
        "                csv_data.append([img_filename_json,1,0,0])\n",
        "                continue\n",
        "\n",
        "\n",
        "            print(\"На изображении не обнаружено животных и нет дефектов. Помещаем в класс Empty.\")\n",
        "            copy_file(full_filename, full_empty_dir, path) # копируем файл в директорию пустых\n",
        "            csv_data.append([img_filename_json,0,1,0])\n",
        "\n",
        "            # cv2.putText(image, \"{}: {:.2f}\".format(text, fm), (30, 30),\n",
        "            # cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "            # cv2.putText(image, \"{}: {:.2f}\".format(\"number_identical_bytes\", number_identical_bytes), (30, 60),\n",
        "            # cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "            # #cv2.imshow(\"Image\", image) # показать на экране\n",
        "            # cv2.imwrite(os.path.join(targetpath, file), image)\n",
        "\n",
        "\n",
        "    with open(os.path.join(targetpath, 'submission.csv'), 'w', encoding='utf-8', newline='') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile, delimiter=',')\n",
        "        csv_writer.writerow(['filename','broken','empty','animal'])\n",
        "        for s in csv_data:\n",
        "            csv_writer.writerow(s)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nThe End\")\n",
        "    print(\"Press any key to continue\")\n",
        "    key = cv2.waitKey(0)\n",
        "\n",
        "def arg_parser():\n",
        "    epilog_text = \"\"\"\n",
        "    Скрипт фильтрации изображений\n",
        "    \"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Определение дефектных фото', epilog=epilog_text)\n",
        "    parser.add_argument('srcpath', metavar='SOURCE_PATH', type=str,\n",
        "                        help='Путь к папке - источнику изображений')\n",
        "    parser.add_argument('targetpath', metavar='TARGET_PATH', type=str,\n",
        "                        help='Путь к целевой папке найденных (дефектных) изображений')\n",
        "    parser.add_argument('json', metavar='JSON_PATH', type=str,\n",
        "                        help='Путь к json файлу с результатами распознавания животных')\n",
        "    parser.add_argument(\"-tb\", \"--threshold_blur\", type=int, default=280,\n",
        "                        help=\"Порог 'размытости' изображений (по умолчанию = 280)\")\n",
        "    parser.add_argument(\"-ni\", \"--threshold_identical_bytes\", type=int, default=65000,\n",
        "                        help=\"Порог 'размытости' изображений (по умолчанию = 65000)\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    global ARG\n",
        "    ARG = arg_parser()"
      ],
      "metadata": {
        "id": "qIpoofpP44vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processing_dataset(images_dir,'/content/итог', output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETfgGgTT5jYh",
        "outputId": "197056b0-7afe-462d-ebd2-b01553dd9aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Фотоловушка Иониха/SYER0023.JPG\n",
            "Размытое изображение variance_of_laplacian: 268.09\n",
            "/content/Фотоловушка Иониха/SYER0024.JPG\n",
            "Размытое изображение variance_of_laplacian: 244.74\n",
            "/content/Фотоловушка Иониха/SYER0025.JPG\n",
            "Размытое изображение variance_of_laplacian: 243.12\n",
            "/content/Фотоловушка Иониха/SYER0026.JPG\n",
            "Обнаружены животные с уверенностью: 0.95\n",
            "/content/Фотоловушка Иониха/SYER0027.JPG\n",
            "Обнаружены животные с уверенностью: 0.95\n",
            "/content/Фотоловушка Иониха/SYER0028.JPG\n",
            "Обнаружены животные с уверенностью: 0.94\n",
            "/content/Фотоловушка Иониха/SYER0029.JPG\n",
            "На изображении не обнаружено животных и нет дефектов. Помещаем в класс Empty.\n",
            "/content/Фотоловушка Иониха/SYER0030.JPG\n",
            "На изображении не обнаружено животных и нет дефектов. Помещаем в класс Empty.\n",
            "/content/Фотоловушка Иониха/SYER0031.JPG\n",
            "На изображении не обнаружено животных и нет дефектов. Помещаем в класс Empty.\n",
            "/content/Фотоловушка Иониха/SYER0032.JPG\n",
            "На изображении не обнаружено животных и нет дефектов. Помещаем в класс Empty.\n",
            "\n",
            "The End\n",
            "Press any key to continue\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}